<!DOCTYPE html>
<html>
<head>
<title>Dave's Life On Hold</title>
<meta name="description" content="Tales from the edge of the future..." />
<meta name="HandheldFriendly" content="True" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

<link rel="stylesheet" type="text/css" href="/style.css" />
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-71410771-1', 'auto');
ga('send', 'pageview');
</script>
</head>
<body>
<div class="content">
<header>
<a href="/index.html"><img class="title" src="/title-2.png"></a>
</header>
<main>
<h1>Many to Many Services</h1>
<p>Over the past two decades, I have spent the majority of my waking hours thinking about how to build applications that bring many users together with many different service providers. Take the case of the poker company, where there was not only many humans playing poker at a time, but also many AIs which managed the tables, ran the bank and managed accounts, and also AIs which played poker as well to flesh out tables (also useful for testing load). In that infrastructure everything revolved around ensuring messages reliably made it to all interested parties, and the servers and clients cooperatively managed load. The smart server/smart client model works spectacularly well when reliability is measured in terms of sub-second outages. <br><br>Designing systems in which failure is the expected mode of operation, it is important to look at opportunities to glean system information without doing additional work. Using a client side timer to specifically test connectivity isn&#39;t a great use of resources, but using a periodic state transfer on a persistent connection with a timeout probably is.  When a disconnect is detected, you don&#39;t want to rely upon a set progression of fail overs, as that will generate a stampede, but rather can connect to a random endpoint and then discover a gathering point. I used this approach in several game clients to avoid the thundering herd problem when an entire datacenter went down. Combined with a retry backoff and a client with a limited memory, individual server failures would produce an even increase of load across the clusters. Then after a brief spike in activity, consistent hashing on the server side would redistribute the clients evenly across the clusters. <br><br>Having a plan for how you respond to failure also involves learning to avoid generating spikes. One of the key statistical tools is randomness. It allows you to flatten load for homogeneous requests. Randomness does not however help you flatten load for heavy requests, and can in fact cause server failure intermittently if it results in some particularly heavy requests to pool at a single endpoint. Part of the logic used in the game&#39;s connection layer relied upon the fact that the initial random request would always generate a redirect as it&#39;s response. On the serverside, the calculation to determine the location of the redirect could be done entirely in memory and in constant time. These two factors were important for balancing the load if more than a single server hosting 200-2000 concurrent connections web down. <br><br>The real smarts of this system came in with setting up the initial load balancing. While clients would hash to a given room, the servers would communicate load via a shared connection, allowing the other servers in the pool to volunteer for the creation of an endpoint. Effectively, every time a client requested a new chat room be created the servers would draw straws weighted by their load, and the shortest straw would host the room. The client would then be redirected to the newly created endpoint, which itself was managed as a separate process on the specified server. <br><br>For rooms which could not fit on a single server, a cohort of servers could pool their connections and used a relay proxy bot to shuffle communications between them. By adding the S2S plumbing, geographically disparate shared spaces could be created, with multiple points of entry. The persistent object store itself worked on this principal, where 3 machines would each join a room and evaluate each transaction on the objects in their address space. Since each object memory was append only, the response consisted of whever got a result first. Any server missing the data would copy down the object when it was accessed. This ensured that as long as one pool member had a copy, all would have a copy if the data was relevant.  Storage pressure was never addressed in so far that the address space was so huge, it was easier to migrate all of the active objects to new partitions and recycle the old spaces. This technique would have failed when the partition space reached half saturation 32k partitions, but that would imply 96k servers each hosting full images. In otherwords, a problem so far removed from the realistic problem space as to not be worth considering. <br><br>And that is probably the key concept in scaling, there exists a point Omega, such that beyond which nothing of value is gained. If your system&#39;s omega is outside the realm of realistic possibilities, your system is said to scale. This also works with an alpha value, usually 1, below which no useful work can be done. But that is a post for another day. </p>
</main>
<footer>
<hr>
<figure class="logo"></figure>
<section id="share" class="share"></section>
<section class="copyright"><a href="mailto:dave@dloh.org">&copy; 2016 David J. Goehrig</a></section>
</footer>
</div>
<script>
	twitter = document.createElement("a")
	twitter.className = "fa fa-twitter"
	twitter.href = "https://twitter.com/intent/tweet?text=Dave%27s%20Life%20On%20Hold&amp;url=" + document.location.href;
	twitter.onclick = function() {
		window.open(this.href, 'twitter-share', 'width=550,height=235');
		return false;
	}
	twitter.innerHtml = '<span class="hidden">Twitter</span>'

	facebook = document.createElement("a")
	facebook.className = "fa fa-facebook"
	facebook.href = "https://www.facebook.com/sharer/sharer.php?u=" + document.location.href;
	facebook.onclick = function() {
		window.open(this.href, 'facebook-share','width=580,height=296');
		return false;
	}
	facebook.innerHtml = '<span class="hidden">Facebook</span></a>'

	goog = document.createElement("a")
	goog.className = "fa fa-google"
	goog.href = "https://plus.google.com/share?url=" + document.location.href;
	goog.onclick = function() {
		window.open(this.href, 'google-plus-share', 'width=490,height=530');
		return false;
	}
	goog.innerHtml = '<span class="hidden">Google+</span></a>'

	sh = document.getElementById("share");
	sh.appendChild(twitter);
	sh.appendChild(facebook);
	sh.appendChild(goog);
</script>
</body>
</html>
